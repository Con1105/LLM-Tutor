# -*- coding: utf-8 -*-
"""KG_Extraction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1p8_Sv1VRvq5lfOOiwRpY5Wj-Yd1vrCMB
"""

# !pip install kg-gen
# !pip install PyMuPDF
# !pip install spacy
# !python -m spacy download en_core_web_sm
# !pip install rdflib
# !pip install plotly
# # Fix nbformat compatibility issue
# !pip install --upgrade nbformat
# !pip install --upgrade plotly
# !pip install --upgrade ipywidgets
# !pip install lxml sympy
# !pip install pygraphviz
# !pip install streamlit networkx plotly
# !pip install streamlit-plotly-events

"""# Libraries"""

# from kg_gen import KGGen
import networkx as nx
import matplotlib.pyplot as plt
import fitz  # PyMuPDF
# from dspy import settings
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
import numpy as np
import networkx as nx
import io
from ner_filtering import process_graph_with_ner
from collections import Counter
import json
from openai import OpenAI
from difflib import get_close_matches
from collections import Counter
# from kg_instance import get_kg_instance
import streamlit as st
# from kg_instance import kg
"""# KGGEN

### Initialisation
"""





# In kg_extraction.py

# @st.cache_resource
# def get_kg():
#     return KGGen(
#         model="openai/gpt-4o",
#         temperature=0.0,
#         api_key="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"
#     )


def find_main_node_triplets(graph):
    """Returns the node with the most outgoing edges (triplet format)."""
    out_degree_counts = Counter()
    for src, _, tgt in graph.relations:
        out_degree_counts[src] += 1
    main_node = out_degree_counts.most_common(1)[0][0] if out_degree_counts else None
    return main_node

client = OpenAI(api_key=st.secrets["OPENAI_API_KEY"])


def augment_graph_with_gpt_entities_relations(graph, pdf_text, client):
    

    prompt = f"""
    You are given a Text and the extracted Entities and Relations for it.
    From the Text, find all the educational concepts or research topics and then find their relations between them or with existing entities in the Entities provided and append them to Entities and Relations correspondingly.
    For example you found Reinforcement Learning in the text, then you found Bellman Equation in the text and there is an Entity called "PPO", then you add Reinforcement Learning and Bellman Equation to the Entities and add the edges ["PPO", "is", "Reinforcement Learning"], ["Reinforcement Learning", "uses", "Bellman Equation"] to the Relations.

    Format your output as JSON like this:
    {{
      "filtered_entities": [...],
      "filtered_relations": [
        ["subject", "predicate", "object"],
        ...
      ]
    }}

    Text:
    {pdf_text}

    Entities:
    {list(graph.entities)}

    Relations:
    {list(graph.relations)}

    Return ONLY the filtered list of entities and the relations (triplets).
    """

    try:
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.0
        )
        raw_response = response.choices[0].message.content.strip()
        print("ğŸ” Raw LLM response:")
        print(raw_response)

        # Extract valid JSON block
        start = raw_response.find('{')
        end = raw_response.rfind('}') + 1
        json_text = raw_response[start:end]

        parsed = json.loads(json_text)
        filtered_entities = set(parsed["filtered_entities"])
        filtered_relations = set(tuple(triplet) for triplet in parsed["filtered_relations"])

        graph.entities = sorted(list(set(graph.entities).union(filtered_entities)))
        graph.relations = sorted(list(set(graph.relations).union(filtered_relations)))

        print("âœ… Parsed and updated graph successfully.")
    except Exception as e:
        print("âŒ Failed to augment graph:", e)




def add_prerequisite_relations(graph, client):
    """
    Uses GPT to infer prerequisite relationships between Entities and append them to the graph's relations.
    Keeps all original relations.
    """
    import json

    prompt = f"""
    You are given Entities and Relations between them.
    For each Entity, look if it has other Entities as a prerequisite for it. If yes, create a Relation as ["subject", "predicate", "object"] where subject is the Entity being processed and object is the prerequisite Entity.
    Check if a relation with the same source and target already exists in Relations provided, if it does not exist, append the new one to the Relations.
    Keep all original relations!

    Format your output as JSON like this:
    {{
      "filtered_relations": [
        ["subject", "predicate", "object"],
        ...
      ]
    }}

    Entities:
    {list(graph.entities)}

    Relations:
    {list(graph.relations)}

    Return ONLY the filtered list of relations (triplets).
    """

    try:
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.0
        )

        raw_response = response.choices[0].message.content.strip()
        print("ğŸ” Raw LLM response:")
        print(raw_response)
        # Extract a valid JSON block
        start = raw_response.find('{')
        end = raw_response.rfind('}') + 1
        json_text = raw_response[start:end]

        parsed = json.loads(json_text)
        new_relations = set(tuple(triplet) for triplet in parsed["filtered_relations"])
        print("New relations ", len(new_relations))

        # Merge with existing relations
        current_relations = set(tuple(r) for r in graph.relations)
        combined_relations = current_relations.union(new_relations)

        graph.relations = sorted(list(combined_relations))

        print("âœ… Prerequisite relations added successfully.")
    except Exception as e:
        print("âŒ Failed to add prerequisite relations:", e)


def convert_entities_to_dependency_dict(graph, client):
    """
    Uses GPT to generate a nested prerequisite dictionary structure from graph.entities.
    This helps visualize learning dependencies between educational concepts.
    """
    import json

    prompt = f"""
    You are given Entities.

    ğŸ“Œ What is a prerequisite?
    A concept **A** is a prerequisite of **B** if:
    - A is almost always taught **before** B in standard university curricula, academic syllabi, or well-established MOOCs (e.g., MIT, Stanford, Coursera).
    - A is essential foundational knowledge needed to understand B.
    - A should **not** be included as a prerequisite if it's merely related or occasionally taught before â€” it must be consistently taught before B.
    
    Your task is to return a single nested dictionary, where:
    - Each **key** is one of the provided entities.
    - The **value** is a dictionary of its direct prerequisite concepts that are **often found in curiculums/courses**. Generate up to 2 prerequisites and only if necessary.
    - Each prerequisite can itself have its own prerequisites, recursively. Do not go too deep unless necessary.
    - If a concept has **no prerequisites**, its value should be `null`.
    - You may include shallow nested prerequisites (e.g., one or two levels), but avoid unnecessary deep recursion.

    Only include concepts that are educational (i.e., found in school/university curricula or well-defined academic resources).

    ğŸ“Œ Example:
    Input: ["Reinforcement Learning", "Machine Learning"]
    Output:
    {{
      "Reinforcement Learning": {{
        "Q-Learning": {{
          "Bellman Equation": null
        }},
        "Markov Decision Process": null,
        "Machine Learning": {{
          "Neural Networks": null,
          "Probability Theory": null
        }}
      }},
      "Gradient Descent": {{
        "Calculus": null
      }}
    }}

    Now, using this format, generate the nested dictionary for the following entities:

    Entities:
    {list(graph.entities)}

    Return ONLY the nested dictionary.
    """

    try:
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.0
        )

        raw_response = response.choices[0].message.content.strip()
        print("ğŸ” Raw LLM response:")
        print(raw_response)
        # Extract a valid JSON block    
        start = raw_response.find('{')
        end = raw_response.rfind('}') + 1
        json_text = raw_response[start:end]

        nested_dependency_dict = json.loads(json_text)

        print("âœ… Dependency dictionary parsed successfully.")
        return nested_dependency_dict

    except Exception as e:
        print("âŒ Failed to generate dependency dictionary:", e)
        return {}


# --- Step 1: Flatten nested dictionary into triplet edges ---
def flatten_prereq_tree(concept, subtree, edges):
    if subtree is None:
        return
    for child, subchild in subtree.items():
        edges.append((concept, "depends on", child))
        flatten_prereq_tree(child, subchild, edges)

def build_edges_from_nested_dict(nested_dict):
    edges = []
    for root, subtree in nested_dict.items():
        flatten_prereq_tree(root, subtree, edges)
    return edges

# --- Step 2: Deduplicate concept names and normalize ---
def deduplicate_entities(edges, threshold=0.75):
    """Deduplicate and normalize entity names"""
    nodes = list(set([src for src, _, tgt in edges] + [tgt for _, _, tgt in edges]))

    node_map = {}
    canonical_nodes = []

    for node in nodes:
        # First, check for exact case-insensitive matches
        case_insensitive_match = None
        for canonical in canonical_nodes:
            if canonical.lower() == node.lower():
                case_insensitive_match = canonical
                break

        if case_insensitive_match:
            # Use the existing canonical form
            node_map[node] = case_insensitive_match
        else:
            # Check for fuzzy matches using get_close_matches
            match = get_close_matches(node, canonical_nodes, n=1, cutoff=threshold)
            if match:
                node_map[node] = match[0]
            else:
                # Create new canonical form - prefer the first occurrence's case
                canonical = node.strip()
                node_map[node] = canonical
                canonical_nodes.append(canonical)

    deduped_edges = []
    for src, rel, tgt in edges:
        new_src = node_map.get(src, src)
        new_tgt = node_map.get(tgt, tgt)
        deduped_edges.append((new_src, rel, new_tgt))

    return list(set(deduped_edges)), set(node_map.values())

# --- Step 3: Integrate with OpenAI response ---
# response.choices[0].message.content is your string from the LLM

# raw_response = response.choices[0].message.content.strip()

# # Safely extract JSON block
# start = raw_response.find('{')
# end = raw_response.rfind('}') + 1
# json_text = raw_response[start:end]

# try:
#     nested_dict = json.loads(json_text)
#     # Flatten
#     raw_edges = build_edges_from_nested_dict(nested_dict)
#     # Deduplicate
#     filtered_relations, filtered_entities = deduplicate_entities(raw_edges)

#     # Use directly with your visualization
#     graph.relations = list(set(graph.relations).union(filtered_relations))

#     graph.entities = list(set(graph.entities).union(filtered_entities))

# except json.JSONDecodeError as e:
#     print("âŒ JSON parsing failed:", e)
#     print("â›”ï¸ Raw content:\n", json_text)


def convert_relations_to_dependency_format(graph, client):
    """
    Converts the graph's semantic triplet relations into dependency format.
    Dependency format: [source, target] means 'source depends on target'.
    Updates graph.relations with standardized dependency edges.
    """
    import json

    prompt = f"""
    You are an expert in understanding semantic relationships between concepts. Your task is to convert a list of relations into standardized dependency format.

    **Dependency Format Rule:**
    - [source, target] means "source depends on target"
    - The source concept requires knowledge or understanding of the target concept
    - Do not change entity names from old relations to standarized dependency ones.

    **Examples:**
    - "PPO uses Gradient Descent" â†’ [PPO, Gradient Descent] (PPO algorithm depends on gradient descent)
    - "Self-Attention used in Transformers" â†’ [Transformers, Self-Attention] (Transformers depend on self-attention mechanism)
    - "Reinforcement Learning part of Machine Learning" â†’ [Reinforcement Learning, Machine Learning] (RL is a subset that depends on ML)
    - "Neural Network contains Layers" â†’ [Layers, Neural Network] (Layers depend on the neural network concept)
    - "Calculus prerequisite for Machine Learning" â†’ [Machine Learning, Calculus] (ML depends on calculus)

    **Relations:**
    {list(graph.relations)}

    **Instructions:**
    1. Analyze each relation semantically
    2. Determine which concept depends on which
    3. Return ONLY a JSON array of [source, target] pairs
    4. Each pair should represent "source depends on target"

    Format your output as JSON like this:
    {{
      "filtered_relations": [
        ["source1", "target1"],
        ["source2", "target2"],
        ...
      ]
    }}

    Think step by step about each relation and determine the dependency direction.
    Return ONLY the JSON.
    """

    try:
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.0
        )

        raw_response = response.choices[0].message.content.strip()
        print("ğŸ” Raw LLM response:")
        print(raw_response)
        # Extract a valid JSON block
        start = raw_response.find('{')
        end = raw_response.rfind('}') + 1
        json_text = raw_response[start:end]

        parsed = json.loads(json_text)
        dependency_relations = set(tuple(doublet) for doublet in parsed["filtered_relations"])

        graph.relations = sorted(list(dependency_relations))

        print("âœ… Relations converted to dependency format successfully.")
    except Exception as e:
        print("âŒ Failed to convert relations to dependency format:", e)


def create_directed_tree_graph_from_graph_object(graph):
    import networkx as nx
    import plotly.graph_objects as go
    from collections import defaultdict, deque

    # Build graph
    G = nx.DiGraph()
    for entity in graph.entities:
        G.add_node(entity)
    for src, tgt in graph.relations:
        G.add_edge(src, tgt)

    # Compute depths (layers) using BFS
    roots = [n for n in G.nodes() if G.in_degree(n) == 0]
    depth_map = {}
    queue = deque([(root, 0) for root in roots])
    while queue:
        node, depth = queue.popleft()
        if node not in depth_map:
            depth_map[node] = depth
            for child in G.successors(node):
                queue.append((child, depth + 1))

    # Group nodes by depth
    layer_nodes = defaultdict(list)
    for node, depth in depth_map.items():
        layer_nodes[depth].append(node)

    # Assign x/y positions manually
    pos = {}
    layer_gap = 2.5
    node_gap = 2.5

    for depth, nodes in layer_nodes.items():
        for i, node in enumerate(nodes):
            x = i * node_gap - (len(nodes) - 1) * node_gap / 2
            y = -depth * layer_gap
            pos[node] = (x, y)

    # Draw nodes with alternating label positions
    # Draw nodes with alternating label positions based on (depth, horizontal position)
    node_positions = [(node, *pos.get(node, (0, 0))) for node in G.nodes()]
    # Sort first by depth (y), then by horizontal x
    node_positions.sort(key=lambda tup: (tup[2], tup[1]))  # sort by (y, x)

    node_x, node_y, node_text, text_pos = [], [], [], []
    for i, (node, x, y) in enumerate(node_positions):
        node_x.append(x)
        node_y.append(y)
        node_text.append(node)
        text_pos.append('top center' if i % 2 == 0 else 'bottom center')



    node_trace = go.Scatter(
        x=node_x,
        y=node_y,
        mode='markers+text',
        text=node_text,
        textposition=text_pos,
        textfont=dict(color='black', size=9),
        marker=dict(color='blue', size=20),
        hoverinfo='text',
        showlegend=False
    )

    # Draw edges with arrows, avoiding perfectly horizontal ones
    annotations = []
    for src, tgt in G.edges():
        x0, y0 = pos[src]
        x1, y1 = pos[tgt]

        # Add vertical or slanted direction by adjusting same-layer edges slightly
        if y0 == y1:
            x1 += 0.2  # small nudge to avoid horizontal line
            y1 -= 0.3

        dx, dy = x1 - x0, y1 - y0
        shrink = 0.1
        x_start = x0 + dx * shrink
        y_start = y0 + dy * shrink
        x_end = x1 - dx * shrink
        y_end = y1 - dy * shrink

        annotations.append(dict(
            ax=x_start, ay=y_start,
            x=x_end, y=y_end,
            xref='x', yref='y',
            axref='x', ayref='y',
            showarrow=True,
            arrowhead=2,
            arrowsize=1.5,
            arrowwidth=1.8,
            arrowcolor='blue',
            opacity=0.8
        ))

    # Create figure
    fig = go.Figure()
    fig.add_trace(node_trace)
    fig.update_layout(
        title='Top-Down Directed Knowledge Graph (Improved Readability)',
        annotations=annotations,
        hovermode='closest',
        dragmode='pan',
        autosize=True,
        width=1200,
        height=1000,
        margin=dict(b=40, l=40, r=40, t=60),
        xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
        yaxis=dict(showgrid=False, zeroline=False, showticklabels=False)
    )

    return fig

# fig = create_directed_tree_graph_from_graph_object(graph)



# Step 1: Remove entities with no edges
def prune_isolated_entities(graph):
    connected = set()
    for src, tgt in graph.relations:
        connected.add(src)
        connected.add(tgt)
    graph.entities = [e for e in graph.entities if e in connected]

# Step 2: Find the "main" node (source of most edges)
def find_main_node_doublets(graph):
    src_counts = Counter(src for src, _ in graph.relations)
    return src_counts.most_common(1)[0][0] if src_counts else None

# Step 3: Connect all top-level nodes to the main node
def connect_to_main_node(graph, main):
    children = set(tgt for _, tgt in graph.relations)
    to_connect = [e for e in graph.entities if e not in children and e != main]
    for e in to_connect:
        graph.relations.append((main, e))


def find_main_node_with_gpt(graph, pdf_text, client):
    """
    Uses GPT to identify the main educational concept (main node) from graph.entities and pdf_text.
    Returns the matched entity from the graph or None if not found.
    """


    prompt = f"""
You are an expert in education and curriculum design.

You are given:
- A list of educational concepts (called 'entities'), extracted from an academic paper.
- The full text of the paper.

Your task:
- Choose ONE main concept that is most central to the paper from the Entities provided â€” the one that most other concepts depend on, expand upon, or relate to.
- Choose the main concept ONLY from the provided entities and return the name exactly as given in the list.

Format your answer strictly as:
{{ "main_concept": "..." }}

Entities:
{graph.entities}

Text:
{pdf_text}

Return only the JSON.
"""

    try:
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.0
        )

        raw_response = response.choices[0].message.content.strip()
        print("ğŸ” Raw LLM response:")
        print(raw_response)
        start = raw_response.find('{')
        end = raw_response.rfind('}') + 1
        json_text = raw_response[start:end]
        parsed = json.loads(json_text)
        main_concept = parsed.get("main_concept", None)

        if not main_concept:
            print("âš ï¸ No main concept returned by GPT.")
            return None

        # Match it case-insensitively to entities in the graph
        match = next((e for e in graph.entities if e.lower() == main_concept.lower()), None)

        if match:
            print(f"âœ… Main concept identified: {match}")
        else:
            print(f"âš ï¸ GPT returned '{main_concept}', which is not found in graph entities.")

        return match

    except Exception as e:
        print("âŒ Failed to identify main concept with GPT:", e)
        return None

# --- Apply all ---

# if main:
#     prune_isolated_entities(graph)
#     connect_to_main_node(graph, main)

#     fig = create_directed_tree_graph_from_graph_object(graph)

import networkx as nx

import networkx as nx

def enforce_dag_and_root(graph, main_node):
    """
    Removes cycles from the graph and ensures the main_node is a root (has no incoming edges).
    
    Parameters:
    - graph: the knowledge graph object with `graph.relations` as (src, tgt) pairs
    - main_node: the node that should be the root (no incoming edges)
    """
    # Step 1: Build graph
    G = nx.DiGraph()
    G.add_nodes_from(graph.entities)
    G.add_edges_from(graph.relations)

    # Step 2: Remove cycles
    try:
        while True:
            cycle = nx.find_cycle(G, orientation="original")
            if not cycle:
                break
            src, tgt, _ = cycle[0]
            print(f"â›” Removing cycle edge: {src} -> {tgt}")
            G.remove_edge(src, tgt)
    except nx.NetworkXNoCycle:
        print("âœ… No cycles found.")

    # Step 3: Ensure main node is a root (remove all incoming edges)
    incoming_to_main = list(G.in_edges(main_node))
    if incoming_to_main:
        print(f"ğŸ”§ Removing incoming edges to main node '{main_node}':")
        for src, tgt in incoming_to_main:
            print(f"â›” Removing edge: {src} -> {tgt}")
            G.remove_edge(src, tgt)
    else:
        print(f"âœ… Main node '{main_node}' is already a root.")

    # Step 4: Update graph
    graph.relations = sorted(list(G.edges()))


def remove_transitive_edges_verbose(graph):
    # Step 1: Build full directed graph
    G = nx.DiGraph()
    G.add_nodes_from(graph.entities)
    G.add_edges_from(graph.relations)

    # Step 2: Check if DAG
    if not nx.is_directed_acyclic_graph(G):
        print("âš ï¸ Graph is not a DAG. Cannot perform transitive reduction.")
        return

    # Step 3: Perform transitive reduction
    reduced_G = nx.transitive_reduction(G)

    # Step 4: Identify removed edges and why
    removed = []
    for src, tgt in G.edges():
        if not reduced_G.has_edge(src, tgt):
            # Find intermediate node(s) showing transitive connection
            for path in nx.all_simple_paths(G, source=src, target=tgt):
                if len(path) > 2:
                    removed.append(((src, tgt), path))
                    break  # Just show one example path

    # Step 5: Report and update
    if removed:
        print("ğŸ” Removed redundant edges:")
        for (src, tgt), path in removed:
            print(f"Removed: ({src} â†’ {tgt}) â€” Covered by path: {' â†’ '.join(path)}")
    else:
        print("âœ… No redundant edges found.")

    graph.relations = sorted(list(reduced_G.edges()))

def extract_kg_from_pdf_bytes(pdf_bytes, kg):
    # Create the KGGen object ONCE at the module level
    # from kg_gen import KGGen  # <== lazy import here avoids thread clash

    # kg = KGGen(
    #     model="openai/gpt-4o",
    #     temperature=0.0,
    #     api_key="sk-..."
    # )
    pdf_stream = io.BytesIO(pdf_bytes)
    doc = fitz.open(stream=pdf_stream, filetype="pdf")
    text = ""
    for page in doc:
        text += page.get_text()

    """### Generate Graph"""
    
    graph = kg.generate(
        input_data=text,
        chunk_size=100000,
        cluster=True,
        context=''
    )
    print("Processing graph with NER tagging and filtering...")
    graph = process_graph_with_ner(graph)
    # main = find_main_node_triplets(graph)
    augment_graph_with_gpt_entities_relations(graph, text, client)
    add_prerequisite_relations(graph, client)
    
    try:
        dependency_tree = convert_entities_to_dependency_dict(graph, client)
        # Flatten
        raw_edges = build_edges_from_nested_dict(dependency_tree)
        # Deduplicate
        filtered_relations, filtered_entities = deduplicate_entities(raw_edges)

        # Use directly with your visualization
        graph.relations = sorted(list(set(graph.relations).union(filtered_relations)))

        graph.entities = sorted(list(set(graph.entities).union(filtered_entities)))

    except json.JSONDecodeError as e:
        print("âŒ JSON parsing failed:", e)

    convert_relations_to_dependency_format(graph, client)
    prune_isolated_entities(graph)
    main = find_main_node_with_gpt(graph, text, client)
    
    enforce_dag_and_root(graph, main_node=main)
    if main in graph.entities:
        connect_to_main_node(graph, main)
        print("âœ… Main node is {main}.")
    remove_transitive_edges_verbose(graph)

    # Convert relations to (src, tgt) pairs if they are triples
    # if graph.relations and len(graph.relations[0]) == 3:
    #     graph.relations = [(src, tgt) for (src, _, tgt) in graph.relations]
    return graph







